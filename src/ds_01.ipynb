{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLP beginer\n",
    "\n",
    "https://github.com/proger-n/DS_01_Tweets/blob/main/src/tweets.ipynb\n",
    "\n",
    "### Introduction\n",
    "NLP (natural language processing) — это область знаний и ряд методов и алгоритмов, которые помогают обрабатывать текстовые данные и извлекать из них информацию. Вы уже работали со структурированными данными — таблицами, полными непрерывных и категориальных признаков. Текст является примером полуструктурированных данных. С ним нужно что-то делать, чтобы использовать его, например, в задачах машинного обучения.\n",
    "\n",
    "ЦЕЛЬ РАБОТЫ - предварительная обработка тектовых данных и обучение с помощью различных классификаторов для решения задачи классификации твитов на три класса (негативный, отрицательный и нейтральный)"
   ],
   "id": "62251287a709a8e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Предварительная обработка текста:\n",
    "Здесь перед подачей в классификатор необходимо предобработать текст, есть следующие способы:\n",
    "- [Term Frequency-Inverse Document Frequency (TF-IDF)](https://habr.com/ru/companies/otus/articles/755772/) - это один из наиболее распространенных и мощных методов для извлечения признаков из текстовых данных. TF-IDF вычисляет важность каждого слова в документе относительно количества его употреблений в данном документе и во всей коллекции текстов. Этот метод позволяет выделить ключевые слова и понять, какие слова имеют больший вес для определенного документа в контексте всей коллекции.\n",
    "Перед тем как вычислять TF-IDF, мы должны выполнить предварительную обработку, такую как удаление стоп-слов, приведение к нижнему регистру и токенизация — разбиение текстов на отдельные слова или токены.\n",
    "- Стемминг (stemming) - это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов. Например слово \"Коты\", станет \"Кот\"\n",
    "- Лематизация - это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.\n",
    "\n",
    "Примеры лематизации и стемминга: Слово good – это лемма для слова better. Стеммер не увидит эту связь, так как здесь нужно сверяться со словарем.\n",
    "Слово play – это базовая форма слова playing. Тут справятся и стемминг, и лемматизация.\n",
    "Слово meeting может быть как нормальной формой существительного, так и формой глагола to meet, в зависимости от контекста.\n",
    "В отличие от стемминга, лемматизация попробует выбрать правильную лемму,опираясь на контекст."
   ],
   "id": "551ed269b1a41b58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0.Import",
   "id": "8a3bea984b74eff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.653514Z",
     "start_time": "2024-06-19T09:59:28.494921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from autocorrect import Speller\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings. filterwarnings('ignore')\n",
    "\n",
    "nltk.download('popular')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Подготовка данных\n",
    "Данные хранятся в архиве. в Архиве три файла csv. В одном файле негативные отзыва, в другом позитивные и в третьем нейтральные. csv файлы содержат одну строку и много столбцов, поэтому нужно будет трансопнировать и привести всё в одну таблицу"
   ],
   "id": "c3084dec204c68f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.840813Z",
     "start_time": "2024-06-19T09:59:29.654392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Распаковываем zip архив\n",
    "with zipfile.ZipFile('../datasets/p00_tweets.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "    \n",
    "# Читаем файлы в pandas\n",
    "positive_tweets = pd.read_csv('data/processedPositive.csv')\n",
    "print(positive_tweets.shape)\n",
    "\n",
    "neutral_tweets = pd.read_csv('data/processedNeutral.csv')\n",
    "print(neutral_tweets.shape)\n",
    "\n",
    "negative_tweets = pd.read_csv('data/processedNegative.csv')\n",
    "print(negative_tweets.shape)"
   ],
   "id": "c07438aadbb8d87e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1186)\n",
      "(0, 1570)\n",
      "(0, 1117)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1. Приведем все данные в одну таблицу.\n",
    "Для этого транспонируем исходные таблицы и назначим предсказательную переменную:\n",
    "- 1 - негативный твит\n",
    "- 2 - нейтральный твит\n",
    "- 3 - положительный твит"
   ],
   "id": "f794af4219eab189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.843443Z",
     "start_time": "2024-06-19T09:59:29.841508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(df, target_index):\n",
    "    df_new = df.T.reset_index().rename(columns={'index': 'tweet'})\n",
    "    df_new['target'] = target_index\n",
    "    return df_new"
   ],
   "id": "3ddfc15396d3770b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.852293Z",
     "start_time": "2024-06-19T09:59:29.844412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "negative_tweets = prepare_data(negative_tweets, target_index=1)\n",
    "neutral_tweets = prepare_data(neutral_tweets, target_index=2)\n",
    "positive_tweets = prepare_data(positive_tweets, target_index=3)\n",
    "\n",
    "df = pd.concat([negative_tweets, neutral_tweets, positive_tweets], ignore_index=True)\n",
    "df"
   ],
   "id": "622dc132b66c68d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  tweet  target\n",
       "0                 How unhappy  some dogs like it though       1\n",
       "1     talking to my over driver about where I'm goin...       1\n",
       "2     Does anybody know if the Rand's likely to fall...       1\n",
       "3            I miss going to gigs in Liverpool unhappy        1\n",
       "4         There isnt a new Riverdale tonight ? unhappy        1\n",
       "...                                                 ...     ...\n",
       "3868  Thanks for the recent follow Happy to connect ...       3\n",
       "3869              - top engaged members this week happy       3\n",
       "3870  ngam to  weeks left for cadet pilot exam cryin...       3\n",
       "3871            Great! You're welcome Josh happy  ^Adam       3\n",
       "3872  Sixth spot not applicable Team! Higher pa! :)K...       3\n",
       "\n",
       "[3873 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>Thanks for the recent follow Happy to connect ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>- top engaged members this week happy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam cryin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Great! You're welcome Josh happy  ^Adam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Sixth spot not applicable Team! Higher pa! :)K...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2. чистим твиты\n",
    "убираем все символы кроме букв и преобразуем к нижнему регистру"
   ],
   "id": "c8773d5415079697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.860796Z",
     "start_time": "2024-06-19T09:59:29.853025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "df = standardize_text(df, 'tweet')\n",
    "df"
   ],
   "id": "657a588af60dfc08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  tweet  target\n",
       "0                 how unhappy  some dogs like it though       1\n",
       "1     talking to my over driver about where im going...       1\n",
       "2     does anybody know if the rands likely to fall ...       1\n",
       "3            i miss going to gigs in liverpool unhappy        1\n",
       "4          there isnt a new riverdale tonight  unhappy        1\n",
       "...                                                 ...     ...\n",
       "3868  thanks for the recent follow happy to connect ...       3\n",
       "3869                top engaged members this week happy       3\n",
       "3870  ngam to  weeks left for cadet pilot exam cryin...       3\n",
       "3871               great youre welcome josh happy  adam       3\n",
       "3872  sixth spot not applicable team higher pa kisse...       3\n",
       "\n",
       "[3873 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where im going...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does anybody know if the rands likely to fall ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there isnt a new riverdale tonight  unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>thanks for the recent follow happy to connect ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>top engaged members this week happy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam cryin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>great youre welcome josh happy  adam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>sixth spot not applicable team higher pa kisse...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3. Токенизация\n",
    "Разбиваем твиты на токены\n",
    "\n",
    "И затем удалим строки где содержаться пустые твиты"
   ],
   "id": "22ec3fad41ee711a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.873547Z",
     "start_time": "2024-06-19T09:59:29.861444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df['tokens'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "df"
   ],
   "id": "9dc5be9961185483",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  tweet  target  \\\n",
       "0                 how unhappy  some dogs like it though       1   \n",
       "1     talking to my over driver about where im going...       1   \n",
       "2     does anybody know if the rands likely to fall ...       1   \n",
       "3            i miss going to gigs in liverpool unhappy        1   \n",
       "4          there isnt a new riverdale tonight  unhappy        1   \n",
       "...                                                 ...     ...   \n",
       "3868  thanks for the recent follow happy to connect ...       3   \n",
       "3869                top engaged members this week happy       3   \n",
       "3870  ngam to  weeks left for cadet pilot exam cryin...       3   \n",
       "3871               great youre welcome josh happy  adam       3   \n",
       "3872  sixth spot not applicable team higher pa kisse...       3   \n",
       "\n",
       "                                                 tokens  \n",
       "0          [how, unhappy, some, dogs, like, it, though]  \n",
       "1     [talking, to, my, over, driver, about, where, ...  \n",
       "2     [does, anybody, know, if, the, rands, likely, ...  \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...  \n",
       "4     [there, isnt, a, new, riverdale, tonight, unha...  \n",
       "...                                                 ...  \n",
       "3868  [thanks, for, the, recent, follow, happy, to, ...  \n",
       "3869         [top, engaged, members, this, week, happy]  \n",
       "3870  [ngam, to, weeks, left, for, cadet, pilot, exa...  \n",
       "3871         [great, youre, welcome, josh, happy, adam]  \n",
       "3872  [sixth, spot, not, applicable, team, higher, p...  \n",
       "\n",
       "[3873 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where im going...</td>\n",
       "      <td>1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does anybody know if the rands likely to fall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[does, anybody, know, if, the, rands, likely, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there isnt a new riverdale tonight  unhappy</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>thanks for the recent follow happy to connect ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>top engaged members this week happy</td>\n",
       "      <td>3</td>\n",
       "      <td>[top, engaged, members, this, week, happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam cryin...</td>\n",
       "      <td>3</td>\n",
       "      <td>[ngam, to, weeks, left, for, cadet, pilot, exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>great youre welcome josh happy  adam</td>\n",
       "      <td>3</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>sixth spot not applicable team higher pa kisse...</td>\n",
       "      <td>3</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.881455Z",
     "start_time": "2024-06-19T09:59:29.874358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_empty_list(cell):\n",
    "    return cell == []\n",
    "\n",
    "mask = df['tokens'].apply(check_empty_list)\n",
    "df = df[~mask]\n",
    "df"
   ],
   "id": "b81087ad3d9b4591",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  tweet  target  \\\n",
       "0                 how unhappy  some dogs like it though       1   \n",
       "1     talking to my over driver about where im going...       1   \n",
       "2     does anybody know if the rands likely to fall ...       1   \n",
       "3            i miss going to gigs in liverpool unhappy        1   \n",
       "4          there isnt a new riverdale tonight  unhappy        1   \n",
       "...                                                 ...     ...   \n",
       "3868  thanks for the recent follow happy to connect ...       3   \n",
       "3869                top engaged members this week happy       3   \n",
       "3870  ngam to  weeks left for cadet pilot exam cryin...       3   \n",
       "3871               great youre welcome josh happy  adam       3   \n",
       "3872  sixth spot not applicable team higher pa kisse...       3   \n",
       "\n",
       "                                                 tokens  \n",
       "0          [how, unhappy, some, dogs, like, it, though]  \n",
       "1     [talking, to, my, over, driver, about, where, ...  \n",
       "2     [does, anybody, know, if, the, rands, likely, ...  \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...  \n",
       "4     [there, isnt, a, new, riverdale, tonight, unha...  \n",
       "...                                                 ...  \n",
       "3868  [thanks, for, the, recent, follow, happy, to, ...  \n",
       "3869         [top, engaged, members, this, week, happy]  \n",
       "3870  [ngam, to, weeks, left, for, cadet, pilot, exa...  \n",
       "3871         [great, youre, welcome, josh, happy, adam]  \n",
       "3872  [sixth, spot, not, applicable, team, higher, p...  \n",
       "\n",
       "[3868 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where im going...</td>\n",
       "      <td>1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does anybody know if the rands likely to fall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[does, anybody, know, if, the, rands, likely, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there isnt a new riverdale tonight  unhappy</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>thanks for the recent follow happy to connect ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>top engaged members this week happy</td>\n",
       "      <td>3</td>\n",
       "      <td>[top, engaged, members, this, week, happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam cryin...</td>\n",
       "      <td>3</td>\n",
       "      <td>[ngam, to, weeks, left, for, cadet, pilot, exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>great youre welcome josh happy  adam</td>\n",
       "      <td>3</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>sixth spot not applicable team higher pa kisse...</td>\n",
       "      <td>3</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3868 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.888335Z",
     "start_time": "2024-06-19T09:59:29.882099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = [word for tokens in df['tokens'] for word in tokens] # Все слова во всех твитах\n",
    "tweet_length = [len(tokens) for tokens in df['tokens']] # список с количеством слов в твите\n",
    "different_words = sorted(list(set(all_words))) # список различных слов\n",
    "print('Всего слов в твитах: ', len(all_words))\n",
    "print('Всего твитов: ', len(df))\n",
    "print('Длина максимального твита: ', max(tweet_length))\n",
    "print('Длина минимального твита: ', min(tweet_length))\n",
    "print('Различных слов во всех твитах: ', len(different_words))"
   ],
   "id": "cbccd1da7785dae0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов в твитах:  33208\n",
      "Всего твитов:  3868\n",
      "Длина максимального твита:  30\n",
      "Длина минимального твита:  1\n",
      "Различных слов во всех твитах:  6378\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4. Функции для трансформорации слова в векторы с использованием различных подходов:\n",
    "- бинаризация, переделка в onehot vector\n",
    "- подсчет слов\n",
    "- TFIDF\n",
    "- Word2Vec\n",
    "\n",
    "Все методы векторизации предназначены для перевода слов в вектора (числа)"
   ],
   "id": "53b999027e51fb4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4.1. Бинаризация\n",
    "про MultiLabelBinarizer() написано [тут](https://ru.stackoverflow.com/questions/928443/%D0%9A%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-sklearn-preprocessing-multilabelbinarizer), [тут](https://spec-zone.ru/scikit_learn/modules/generated/sklearn.preprocessing.multilabelbinarizer) и [тут](https://scikit-learn.ru/1-12-multiclass-and-multioutput-algorithms/#multiclass-classification). В целом данный класс преобразует столбец с токенами в One-Hot-Encoded"
   ],
   "id": "9e4843ee8da245e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.891634Z",
     "start_time": "2024-06-19T09:59:29.888963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_tokens_and_split(df, tokens_column_name, target_column_name, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Преобразует матрицу с токенами в one-hot-векторы и разбиват на тренировочную и тестовую\n",
    "    :param df: датафрейм с токенами и целевой переменной\n",
    "    :param tokens_column_name: название столбца датафрейма с токенами\n",
    "    :param target_column_name: название столбца датафрейма с целевой переменной\n",
    "    :param test_size: размер тестовой выборки\n",
    "    :return: X_train_bin, X_test_bin, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[tokens_column_name],\n",
    "                                                        df[target_column_name],\n",
    "                                                        stratify=df[target_column_name],\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(X_train)  # получает классы - один класс - одно слово, где слова не повторяются\n",
    "    X_train_bin = mlb.transform(X_train) # преобразуем в onehot vector\n",
    "    X_test_bin = mlb.transform(X_test)  # преобразуем в onehot vector\n",
    "    return X_train_bin, X_test_bin, y_train, y_test"
   ],
   "id": "9c5d2084ec6bc14b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4.2. подсчет количества слов\n",
    "про CountVectorizer() можно ознакомится [тута](https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/vectorizers_tutorial_mvsamsonov.ipynb), [тута](https://habr.com/ru/articles/702626/) и [тут](https://habr.com/ru/articles/205360/). \n",
    "Если коротко то данный класс преобразует твиты в матрицу значениями которой, являются количества вхождения данного ключа(слова) в текст. Следует отметить что здесь нужно передать не список токенов, а список предложений. По умолчанию возвращается разреженная матрица"
   ],
   "id": "d03c1b6938729cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.895623Z",
     "start_time": "2024-06-19T09:59:29.892990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_words_and_split(df, tokens_column_name, target_column_name, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Преобразует матрицу с токенами сначала в матрице предложений,\n",
    "    Затем заносит в матрицу для каждого слова - сколько раз оно встречалось во всей выборке\n",
    "    Возвращает разреженную матрицу\n",
    "    \n",
    "    :param df: датафрейм с токенами и целевой переменной\n",
    "    :param tokens_column_name: название столбца датафрейма с токенами\n",
    "    :param target_column_name: название столбца датафрейма с целевой переменной\n",
    "    :param test_size: размер тестовой выборки\n",
    "    :return: X_train_counts, X_test_counts, y_train, y_test\n",
    "    \"\"\"\n",
    "    list_corpus = df[tokens_column_name].apply(lambda x: ' '.join(x)).tolist() # из токенов получаем как бы список твитов в одном предложении\n",
    "    list_labels = df[target_column_name].tolist() # преобразуем целевые переменные в список\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, stratify=list_labels, test_size=test_size, random_state=21)\n",
    "    \n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train_counts = count_vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test_counts = count_vectorizer.transform(X_test).toarray()\n",
    "    \n",
    "    return X_train_counts, X_test_counts, y_train, y_test\n",
    "\n",
    "    "
   ],
   "id": "68d6432e2021a520",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4.3. TFIDF\n",
    "про TFIDF написано [тут](https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/vectorizers_tutorial_mvsamsonov.ipynb)\n",
    "Если слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения CountVectorizer различий не будет. Здесь на помощь приходит TFIDF.\n",
    "\n",
    "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки."
   ],
   "id": "8a9473b4abdfcde1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.898636Z",
     "start_time": "2024-06-19T09:59:29.896187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tfidf_and_split(df, tokens_column_name, target_column_name, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Преобазует матрицу с токенами в TFIDF\n",
    "    \n",
    "    :param df: датафрейм с токенами и целевой переменной\n",
    "    :param tokens_column_name: название столбца датафрейма с токенами\n",
    "    :param target_column_name: название столбца датафрейма с целевой переменной\n",
    "    :param test_size: размер тестовой выборки\n",
    "    :return: X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "    \"\"\"\n",
    "    list_corpus = df[tokens_column_name].apply(lambda x: ' '.join(x)).tolist()\n",
    "    list_labels = df[target_column_name].tolist()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, stratify=list_labels, test_size=test_size, random_state=21)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(X_train)\n",
    "    X_train_tfidf = tfidf.transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test"
   ],
   "id": "1db60f1726f18893",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4.4. Word2Wec\n",
    "Про то как работает Word2Vec можно посмотреть [тут](https://habr.com/ru/articles/585838/) и [тут](https://habr.com/ru/articles/446530/)"
   ],
   "id": "cc165d99bb906c0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.903010Z",
     "start_time": "2024-06-19T09:59:29.899193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def word2vec_and_split(df: pd.DataFrame, tokens_column: str, target_column: str, vector_size: int = 80, min_count: int = 1,\n",
    "                 window: int = 5, epochs: int = 80) -> pd.DataFrame:\n",
    "    \"\"\"The method return a \"word2vec\" column that contains the word2vec representation,\n",
    "    text must be misspelled\"\"\"\n",
    "\n",
    "    sentences = [text for text in df[tokens_column]]\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, min_count=min_count,\n",
    "                     window=window, epochs=epochs)\n",
    "\n",
    "    word2vec_vectors = []\n",
    "    for text in sentences:\n",
    "        if len(text) > 0:\n",
    "            vector = sum(model.wv[word] for word in text if word in model.wv) / len(text)\n",
    "        else:\n",
    "            vector = [0] * vector_size\n",
    "        word2vec_vectors.append(vector)\n",
    "\n",
    "    # df[\"word2vec\"] = word2vec_vectors\n",
    "    # list_corpus = df[\"word2vec\"].tolist()\n",
    "    list_labels = df[target_column].tolist()\n",
    "\n",
    "    X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_vectors, list_labels, stratify=list_labels, test_size=0.2, random_state=21)\n",
    "\n",
    "    return X_train_w2v, X_test_w2v, y_train, y_test"
   ],
   "id": "b3eef59d495a00e1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5. Функции для препроцессинга слов перед векторизацией",
   "id": "cef0fba3feeec336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.905738Z",
     "start_time": "2024-06-19T09:59:29.903678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemming(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    Применяет лематизацию к токенам\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца к которому нужно применить лематизацию\n",
    "    :return: столбец с привденными леммами\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return df[tokens_column_name].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ],
   "id": "c2ac761be28a8777",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.908619Z",
     "start_time": "2024-06-19T09:59:29.906526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stemming(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    Применяет стемминг к токенам\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца к которому нужно применить стемминг\n",
    "    :return: столбец с привденными формами слов\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return df[tokens_column_name].apply(lambda x: [stemmer.stem(word) for word in x])"
   ],
   "id": "134b0d42af220138",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.911199Z",
     "start_time": "2024-06-19T09:59:29.909212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stop_words_remove(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    удаляет стоп слова\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца в котором нужно удалить стоп слова\n",
    "    :return: столбец с удаленными стоп словами\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return df[tokens_column_name].apply(lambda x: [word for word in x if word not in stop_words])"
   ],
   "id": "d93b91ce57e219a2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.913661Z",
     "start_time": "2024-06-19T09:59:29.911736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def misspelling(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    Исправляет орфографические ошибки в словах\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца в котором нужно исправить орфографические ошибки\n",
    "    :return: столбец с исправлеными токенами\n",
    "    \"\"\"\n",
    "    speller = Speller(lang=\"en\")\n",
    "    return df[tokens_column_name].apply(lambda words: [speller(word) for word in words])"
   ],
   "id": "668b475d104e4824",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.6. Вспомогатльная функция для заполнения таблицы",
   "id": "b3ebb61408457588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:59:29.932690Z",
     "start_time": "2024-06-19T09:59:29.914232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_classification(prep_name, df, tokens_column, target_column, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param prep_name: название преподготовки\n",
    "    :param df: dataframe c признаками и целевой переменной\n",
    "    :param tokens_column: название колнки с цлевой переменной\n",
    "    :param target_column: название колонки с целевыми признаками (токенами)\n",
    "    :param model: необученная модель с параметрами\n",
    "    :return: точность предсказаний на тестовой выборке\n",
    "    \"\"\"\n",
    "    result = [prep_name]\n",
    "    X_train, X_test, y_train, y_test = binarize_tokens_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = count_words_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = tfidf_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = word2vec_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "\n",
    "    return result"
   ],
   "id": "dee8bf5d5c7c3a7d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. ML-алгоритмы\n",
    "Поскольк нам дали возможность выбирать модели самим, будем пробовать четыре модели:\n",
    "- логистическая регрессия\n",
    "- дерево решений\n",
    "- случайный лес\n",
    "- метод опорных векторов\n",
    "\n",
    "Так же попробуем использовать GridSeach для выбора оптиального параметра для каждо модели.\n",
    "Поскольку есть много вариаций для которой можно применить GreadSearch, мы сначала применим его для TFIDF с предварительным препроцессингом датасета (стеминг лематизация и удаления стоп слов)\n",
    "Затем выявим лучшие параметры и уже модель с этими параметрами применим ко всем вариациям согласно заданию и заполним таблицы для каждой модели."
   ],
   "id": "b2dd8155a209382a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.0. Для наглядности как работают лематизация, стемминг, исправление ошибок и удаление стоп слов - создадим новый датафрейм\n",
    "Посмотрим как выглядит столбец с токенами после каждой из данных процедур"
   ],
   "id": "ad3571f98c21f710"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:00:44.665466Z",
     "start_time": "2024-06-19T09:59:29.933569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_temp = df[['target', 'tokens']]\n",
    "df_temp['stemming'] = stemming(df_temp, 'tokens')\n",
    "df_temp['lemming'] = lemming(df_temp, 'tokens')\n",
    "df_temp['stemming_misspeling'] = misspelling(df_temp, 'stemming')\n",
    "df_temp['lemming_misspeling'] = misspelling(df_temp, 'lemming')\n",
    "df_temp['stopWords'] = stop_words_remove(df_temp, 'tokens')\n",
    "df_temp['lemming_misspeling_stopWords'] = stop_words_remove(df_temp, 'lemming_misspeling')\n",
    "\n",
    "df_temp"
   ],
   "id": "7de3b736b863bb1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      target                                             tokens  \\\n",
       "0          1       [how, unhappy, some, dogs, like, it, though]   \n",
       "1          1  [talking, to, my, over, driver, about, where, ...   \n",
       "2          1  [does, anybody, know, if, the, rands, likely, ...   \n",
       "3          1  [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4          1  [there, isnt, a, new, riverdale, tonight, unha...   \n",
       "...      ...                                                ...   \n",
       "3868       3  [thanks, for, the, recent, follow, happy, to, ...   \n",
       "3869       3         [top, engaged, members, this, week, happy]   \n",
       "3870       3  [ngam, to, weeks, left, for, cadet, pilot, exa...   \n",
       "3871       3         [great, youre, welcome, josh, happy, adam]   \n",
       "3872       3  [sixth, spot, not, applicable, team, higher, p...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, im,...   \n",
       "2     [doe, anybodi, know, if, the, rand, like, to, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, unhappi]   \n",
       "...                                                 ...   \n",
       "3868  [thank, for, the, recent, follow, happi, to, c...   \n",
       "3869             [top, engag, member, thi, week, happi]   \n",
       "3870  [ngam, to, week, left, for, cadet, pilot, exam...   \n",
       "3871           [great, your, welcom, josh, happi, adam]   \n",
       "3872  [sixth, spot, not, applic, team, higher, pa, k...   \n",
       "\n",
       "                                                lemming  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, likely, to...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, unha...   \n",
       "...                                                 ...   \n",
       "3868  [thanks, for, the, recent, follow, happy, to, ...   \n",
       "3869          [top, engaged, member, this, week, happy]   \n",
       "3870  [ngam, to, week, left, for, cadet, pilot, exam...   \n",
       "3871         [great, youre, welcome, josh, happy, adam]   \n",
       "3872  [sixth, spot, not, applicable, team, higher, p...   \n",
       "\n",
       "                                    stemming_misspeling  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, im,...   \n",
       "2     [doe, anybody, know, if, the, rand, like, to, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, reversal, tonight, unhappy]   \n",
       "...                                                 ...   \n",
       "3868  [thank, for, the, recent, follow, happy, to, c...   \n",
       "3869            [top, engage, member, thi, week, happy]   \n",
       "3870  [nam, to, week, left, for, cadet, pilot, exam,...   \n",
       "3871          [great, your, welcome, josh, happy, adam]   \n",
       "3872  [sixth, spot, not, applied, team, higher, pa, ...   \n",
       "\n",
       "                                     lemming_misspeling  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, likely, to...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, unha...   \n",
       "...                                                 ...   \n",
       "3868  [thanks, for, the, recent, follow, happy, to, ...   \n",
       "3869          [top, engaged, member, this, week, happy]   \n",
       "3870  [nam, to, week, left, for, cadet, pilot, exam,...   \n",
       "3871         [great, youre, welcome, josh, happy, adam]   \n",
       "3872  [sixth, spot, not, applicable, team, higher, p...   \n",
       "\n",
       "                                              stopWords  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, im, goinghe, said, hed, love...   \n",
       "2     [anybody, know, rands, likely, fall, dollar, g...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4              [isnt, new, riverdale, tonight, unhappy]   \n",
       "...                                                 ...   \n",
       "3868  [thanks, recent, follow, happy, connect, happy...   \n",
       "3869               [top, engaged, members, week, happy]   \n",
       "3870  [ngam, weeks, left, cadet, pilot, exam, crying...   \n",
       "3871         [great, youre, welcome, josh, happy, adam]   \n",
       "3872  [sixth, spot, applicable, team, higher, pa, ki...   \n",
       "\n",
       "                           lemming_misspeling_stopWords  \n",
       "0                          [unhappy, dog, like, though]  \n",
       "1     [talking, driver, im, going, said, hed, love, ...  \n",
       "2     [doe, anybody, know, rand, likely, fall, dolla...  \n",
       "3                [miss, going, gig, liverpool, unhappy]  \n",
       "4              [isnt, new, riverdale, tonight, unhappy]  \n",
       "...                                                 ...  \n",
       "3868  [thanks, recent, follow, happy, connect, happy...  \n",
       "3869                [top, engaged, member, week, happy]  \n",
       "3870    [nam, week, left, cadet, pilot, exam, cry, joy]  \n",
       "3871         [great, youre, welcome, josh, happy, adam]  \n",
       "3872  [sixth, spot, applicable, team, higher, pa, ki...  \n",
       "\n",
       "[3868 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemming</th>\n",
       "      <th>stemming_misspeling</th>\n",
       "      <th>lemming_misspeling</th>\n",
       "      <th>stopWords</th>\n",
       "      <th>lemming_misspeling_stopWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, im,...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, im,...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, im, goinghe, said, hed, love...</td>\n",
       "      <td>[talking, driver, im, going, said, hed, love, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[does, anybody, know, if, the, rands, likely, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, like, to, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, likely, to...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, like, to, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, likely, to...</td>\n",
       "      <td>[anybody, know, rands, likely, fall, dollar, g...</td>\n",
       "      <td>[doe, anybody, know, rand, likely, fall, dolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liverpool, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, unhappi]</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "      <td>[there, isnt, a, new, reversal, tonight, unhappy]</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, unhappy]</td>\n",
       "      <td>[isnt, new, riverdale, tonight, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>3</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, ...</td>\n",
       "      <td>[thank, for, the, recent, follow, happi, to, c...</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, ...</td>\n",
       "      <td>[thank, for, the, recent, follow, happy, to, c...</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, ...</td>\n",
       "      <td>[thanks, recent, follow, happy, connect, happy...</td>\n",
       "      <td>[thanks, recent, follow, happy, connect, happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>3</td>\n",
       "      <td>[top, engaged, members, this, week, happy]</td>\n",
       "      <td>[top, engag, member, thi, week, happi]</td>\n",
       "      <td>[top, engaged, member, this, week, happy]</td>\n",
       "      <td>[top, engage, member, thi, week, happy]</td>\n",
       "      <td>[top, engaged, member, this, week, happy]</td>\n",
       "      <td>[top, engaged, members, week, happy]</td>\n",
       "      <td>[top, engaged, member, week, happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>3</td>\n",
       "      <td>[ngam, to, weeks, left, for, cadet, pilot, exa...</td>\n",
       "      <td>[ngam, to, week, left, for, cadet, pilot, exam...</td>\n",
       "      <td>[ngam, to, week, left, for, cadet, pilot, exam...</td>\n",
       "      <td>[nam, to, week, left, for, cadet, pilot, exam,...</td>\n",
       "      <td>[nam, to, week, left, for, cadet, pilot, exam,...</td>\n",
       "      <td>[ngam, weeks, left, cadet, pilot, exam, crying...</td>\n",
       "      <td>[nam, week, left, cadet, pilot, exam, cry, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>3</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, your, welcom, josh, happi, adam]</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, your, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>3</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, p...</td>\n",
       "      <td>[sixth, spot, not, applic, team, higher, pa, k...</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, p...</td>\n",
       "      <td>[sixth, spot, not, applied, team, higher, pa, ...</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, p...</td>\n",
       "      <td>[sixth, spot, applicable, team, higher, pa, ki...</td>\n",
       "      <td>[sixth, spot, applicable, team, higher, pa, ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3868 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Для того что бы ориентировочно прикинуть лучшие параметры для четырех алгоритмов (логистическая регрессия, дерево решений, случайный лес, метод опорных векторов)\n",
    "выберем примерно лучшие параметры для каждого алгоритма с использованием GreadSearchCV.\n",
    "В качестве преобразователя слов в вектора возмем TFIDF.\n",
    "А в качестве признаков возмем твиты после лематизации + исправление ошибок (misspelling) + удаление стоп-слов"
   ],
   "id": "79c2d1ff8a888298"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:00:44.696609Z",
     "start_time": "2024-06-19T10:00:44.666083Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_tf, X_test_tf, y_train, y_test = tfidf_and_split(df_temp, target_column_name='target', tokens_column_name='lemming_misspeling_stopWords')",
   "id": "4422d2d2e7ad01d3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.0.1. Для начала предварительно выберем \"наилучшие\" параметры для каждой модели",
   "id": "e9a4b6e7d2773166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:01:06.655021Z",
     "start_time": "2024-06-19T10:00:44.697382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ\n",
    "clf = LogisticRegression()\n",
    "param_grid = {'C': [0.1, 1, 10, 30, 50],\n",
    "              'solver': ['newton-cg', 'saga'],\n",
    "              'penalty': ['l2', 'l1'],\n",
    "              'max_iter': [3000],\n",
    "              'random_state': [21],\n",
    "              'class_weight': ['balanced'],\n",
    "              'multi_class': ['multinomial'],\n",
    "              'n_jobs': [-1]\n",
    "              }\n",
    "gs = GridSearchCV(clf, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "874977c210a03c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'C': 1, 'class_weight': 'balanced', 'max_iter': 3000, 'multi_class': 'multinomial', 'n_jobs': -1, 'penalty': 'l2', 'random_state': 21, 'solver': 'saga'}\n",
      "лучший score: 0.879118109906886\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:01:10.843190Z",
     "start_time": "2024-06-19T10:01:06.655836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. ДЕРЕВО РЕШЕНИЙ\n",
    "tree = DecisionTreeClassifier()\n",
    "param_grid = {'criterion': ['gini','entropy'],\n",
    "              'max_depth': np.arange(1, 50),\n",
    "              'class_weight': ['balanced', None],\n",
    "              'random_state': [21]\n",
    "            }\n",
    "gs = GridSearchCV(tree, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "e3739224689066c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'class_weight': None, 'criterion': 'gini', 'max_depth': 48, 'random_state': 21}\n",
      "лучший score: 0.8648953578953421\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:04:25.530684Z",
     "start_time": "2024-06-19T10:01:10.843931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Случайный лес\n",
    "forest = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [5, 10, 50, 100, 200, 300],\n",
    "              'criterion': ['gini','entropy'],\n",
    "              'max_depth': np.arange(1, 50, 2),\n",
    "              'class_weight': ['balanced', None],\n",
    "              'random_state': [21]\n",
    "              }\n",
    " \n",
    "gs = GridSearchCV(forest, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "b1df8f24ed221285",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 41, 'n_estimators': 200, 'random_state': 21}\n",
      "лучший score: 0.8668318772840629\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:04:40.175050Z",
     "start_time": "2024-06-19T10:04:25.531426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "svc = SVC()\n",
    "param_grid = {'C': [0.01, 0.1, 1, 1.5, 5, 10],\n",
    "              'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "              'gamma': ['scale', 'auto'],\n",
    "              'class_weight': ['balanced', None],\n",
    "              'random_state': [21]\n",
    "              }\n",
    "gs = GridSearchCV(svc, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "21776de3e182d5d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'C': 1, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear', 'random_state': 21}\n",
      "лучший score: 0.8794417345023552\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Теперь когда мы получили примерно оптимальные гипперпараметры моежм приступить к следующему\n",
    "В следующем шаге мы будем использовать именно \"лучшие\" параметры для каждой модели"
   ],
   "id": "7f76e90436addcd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Логистическая регрессия",
   "id": "62bf5de25abe41ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:07:24.961118Z",
     "start_time": "2024-06-19T10:04:40.175793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "clf = LogisticRegression(C=1,\n",
    "                         class_weight='balanced',\n",
    "                         max_iter=3000,\n",
    "                         solver='saga',\n",
    "                         multi_class='multinomial',\n",
    "                         n_jobs=-1,\n",
    "                         random_state=21,\n",
    "                         penalty='l2',)\n",
    "\n",
    "# Создаем датафрейм кудла будем заносить метрики\n",
    "results_logreg = pd.DataFrame({'preprocces':[], '0 or 1 if the word exists': [], 'word counts':[], 'TFIDF':[], 'Word2Vec':[]})\n",
    "\n",
    "# Заполняем табличку\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('just tokenization', df_temp, 'tokens', 'target', clf)\n",
    "print(\"1 из 7. Расчет точности без препроцессинга завершен\")\n",
    "\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('stemming', df_temp, 'stemming', 'target', clf)\n",
    "print(\"2 из 7. Расчет точности после стемминга завершен\")\n",
    "\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('lemmatization', df_temp, 'lemming', 'target', clf)\n",
    "print(\"3 из 7. Расчет точности после лематизации завершен\")\n",
    "\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('stemming+misspellings', df_temp, 'stemming_misspeling', 'target', clf)\n",
    "print(\"4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\")\n",
    "\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('lemmatization+misspellings', df_temp, 'lemming_misspeling', 'target', clf)\n",
    "print(\"5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\")\n",
    "\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('stopWords', df_temp, 'stopWords', 'target', clf)\n",
    "print(\"6 из 7. Расчет точности после удаления стоп-слов завершен\")\n",
    "\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('lemmatization+misspellings+stopWords', df_temp, 'lemming_misspeling_stopWords', 'target', clf)\n",
    "print(\"7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\")"
   ],
   "id": "75deba7f9bed2e23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 из 7. Расчет точности без препроцессинга завершен\n",
      "2 из 7. Расчет точности после стемминга завершен\n",
      "3 из 7. Расчет точности после лематизации завершен\n",
      "4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\n",
      "5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\n",
      "6 из 7. Расчет точности после удаления стоп-слов завершен\n",
      "7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\n",
      "CPU times: user 2min 52s, sys: 13.5 s, total: 3min 6s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:07:24.966662Z",
     "start_time": "2024-06-19T10:07:24.961851Z"
    }
   },
   "cell_type": "code",
   "source": "results_logreg",
   "id": "af4dbeb6146cd960",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             preprocces  0 or 1 if the word exists  \\\n",
       "0                     just tokenization                   0.910853   \n",
       "1                              stemming                   0.909561   \n",
       "2                         lemmatization                   0.905685   \n",
       "3                 stemming+misspellings                   0.909561   \n",
       "4            lemmatization+misspellings                   0.906977   \n",
       "5                             stopWords                   0.886305   \n",
       "6  lemmatization+misspellings+stopWords                   0.886305   \n",
       "\n",
       "   word counts     TFIDF  Word2Vec  \n",
       "0     0.887597  0.886305  0.878553  \n",
       "1     0.894057  0.890181  0.856589  \n",
       "2     0.888889  0.886305  0.872093  \n",
       "3     0.892765  0.891473  0.855297  \n",
       "4     0.887597  0.885013  0.881137  \n",
       "5     0.887597  0.885013  0.875969  \n",
       "6     0.879845  0.872093  0.854005  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocces</th>\n",
       "      <th>0 or 1 if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just tokenization</td>\n",
       "      <td>0.910853</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.886305</td>\n",
       "      <td>0.878553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemming</td>\n",
       "      <td>0.909561</td>\n",
       "      <td>0.894057</td>\n",
       "      <td>0.890181</td>\n",
       "      <td>0.856589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatization</td>\n",
       "      <td>0.905685</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.886305</td>\n",
       "      <td>0.872093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stemming+misspellings</td>\n",
       "      <td>0.909561</td>\n",
       "      <td>0.892765</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.855297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatization+misspellings</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.885013</td>\n",
       "      <td>0.881137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopWords</td>\n",
       "      <td>0.886305</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.885013</td>\n",
       "      <td>0.875969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatization+misspellings+stopWords</td>\n",
       "      <td>0.886305</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.854005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Дерево решений",
   "id": "505858d5add9e756"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:07:46.282706Z",
     "start_time": "2024-06-19T10:07:24.967271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "tree = DecisionTreeClassifier(class_weight=None,\n",
    "                              criterion='gini',\n",
    "                              max_depth=48,\n",
    "                              random_state=21)\n",
    "\n",
    "# Создаем датафрейм кудла будем заносить метрики\n",
    "results_tree = pd.DataFrame({'preprocces':[], '0 or 1 if the word exists': [], 'word counts':[], 'TFIDF':[], 'Word2Vec':[]})\n",
    "\n",
    "# Заполняем табличку\n",
    "results_tree.loc[len(results_tree)] = make_classification('just tokenization', df_temp, 'tokens', 'target', tree)\n",
    "print(\"1 из 7. Расчет точности без препроцессинга завершен\")\n",
    "\n",
    "results_tree.loc[len(results_tree)] = make_classification('stemming', df_temp, 'stemming', 'target', tree)\n",
    "print(\"2 из 7. Расчет точности после стемминга завершен\")\n",
    "\n",
    "results_tree.loc[len(results_tree)] = make_classification('lemmatization', df_temp, 'lemming', 'target', tree)\n",
    "print(\"3 из 7. Расчет точности после лематизации завершен\")\n",
    "\n",
    "results_tree.loc[len(results_tree)] = make_classification('stemming+misspellings', df_temp, 'stemming_misspeling', 'target', tree)\n",
    "print(\"4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\")\n",
    "\n",
    "results_tree.loc[len(results_tree)] = make_classification('lemmatization+misspellings', df_temp, 'lemming_misspeling', 'target', tree)\n",
    "print(\"5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\")\n",
    "\n",
    "results_tree.loc[len(results_tree)] = make_classification('stopWords', df_temp, 'stopWords', 'target', tree)\n",
    "print(\"6 из 7. Расчет точности после удаления стоп-слов завершен\")\n",
    "\n",
    "results_tree.loc[len(results_tree)] = make_classification('lemmatization+misspellings+stopWords', df_temp, 'lemming_misspeling_stopWords', 'target', tree)\n",
    "print(\"7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\")"
   ],
   "id": "885559e3438213b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 из 7. Расчет точности без препроцессинга завершен\n",
      "2 из 7. Расчет точности после стемминга завершен\n",
      "3 из 7. Расчет точности после лематизации завершен\n",
      "4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\n",
      "5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\n",
      "6 из 7. Расчет точности после удаления стоп-слов завершен\n",
      "7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\n",
      "CPU times: user 25.8 s, sys: 460 ms, total: 26.3 s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:07:46.288314Z",
     "start_time": "2024-06-19T10:07:46.283645Z"
    }
   },
   "cell_type": "code",
   "source": "results_tree",
   "id": "cfd68c36858471a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             preprocces  0 or 1 if the word exists  \\\n",
       "0                     just tokenization                   0.894057   \n",
       "1                              stemming                   0.881137   \n",
       "2                         lemmatization                   0.892765   \n",
       "3                 stemming+misspellings                   0.891473   \n",
       "4            lemmatization+misspellings                   0.887597   \n",
       "5                             stopWords                   0.873385   \n",
       "6  lemmatization+misspellings+stopWords                   0.874677   \n",
       "\n",
       "   word counts     TFIDF  Word2Vec  \n",
       "0     0.874677  0.855297  0.771318  \n",
       "1     0.874677  0.850129  0.757106  \n",
       "2     0.875969  0.865633  0.748062  \n",
       "3     0.861757  0.855297  0.731266  \n",
       "4     0.869509  0.861757  0.733850  \n",
       "5     0.870801  0.874677  0.815245  \n",
       "6     0.866925  0.868217  0.781654  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocces</th>\n",
       "      <th>0 or 1 if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just tokenization</td>\n",
       "      <td>0.894057</td>\n",
       "      <td>0.874677</td>\n",
       "      <td>0.855297</td>\n",
       "      <td>0.771318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemming</td>\n",
       "      <td>0.881137</td>\n",
       "      <td>0.874677</td>\n",
       "      <td>0.850129</td>\n",
       "      <td>0.757106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatization</td>\n",
       "      <td>0.892765</td>\n",
       "      <td>0.875969</td>\n",
       "      <td>0.865633</td>\n",
       "      <td>0.748062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stemming+misspellings</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.861757</td>\n",
       "      <td>0.855297</td>\n",
       "      <td>0.731266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatization+misspellings</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.869509</td>\n",
       "      <td>0.861757</td>\n",
       "      <td>0.733850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopWords</td>\n",
       "      <td>0.873385</td>\n",
       "      <td>0.870801</td>\n",
       "      <td>0.874677</td>\n",
       "      <td>0.815245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatization+misspellings+stopWords</td>\n",
       "      <td>0.874677</td>\n",
       "      <td>0.866925</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.781654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3. Случайный лес",
   "id": "ad394f8ec9c670c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:09:15.334257Z",
     "start_time": "2024-06-19T10:07:46.290224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "forest = RandomForestClassifier(class_weight='balanced',\n",
    "                                criterion='gini',\n",
    "                                max_depth=41,\n",
    "                                random_state=21,\n",
    "                                n_estimators=200,)\n",
    "\n",
    "# Создаем датафрейм кудла будем заносить метрики\n",
    "results_forest = pd.DataFrame({'preprocces':[], '0 or 1 if the word exists': [], 'word counts':[], 'TFIDF':[], 'Word2Vec':[]})\n",
    "\n",
    "# Заполняем табличку\n",
    "results_forest.loc[len(results_forest)] = make_classification('just tokenization', df_temp, 'tokens', 'target', forest)\n",
    "print(\"1 из 7. Расчет точности без препроцессинга завершен\")\n",
    "\n",
    "results_forest.loc[len(results_forest)] = make_classification('stemming', df_temp, 'stemming', 'target', forest)\n",
    "print(\"2 из 7. Расчет точности после стемминга завершен\")\n",
    "\n",
    "results_forest.loc[len(results_forest)] = make_classification('lemmatization', df_temp, 'lemming', 'target', forest)\n",
    "print(\"3 из 7. Расчет точности после лематизации завершен\")\n",
    "\n",
    "results_forest.loc[len(results_forest)] = make_classification('stemming+misspellings', df_temp, 'stemming_misspeling', 'target', forest)\n",
    "print(\"4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\")\n",
    "\n",
    "results_forest.loc[len(results_forest)] = make_classification('lemmatization+misspellings', df_temp, 'lemming_misspeling', 'target', forest)\n",
    "print(\"5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\")\n",
    "\n",
    "results_forest.loc[len(results_forest)] = make_classification('stopWords', df_temp, 'stopWords', 'target', forest)\n",
    "print(\"6 из 7. Расчет точности после удаления стоп-слов завершен\")\n",
    "\n",
    "results_forest.loc[len(results_forest)] = make_classification('lemmatization+misspellings+stopWords', df_temp, 'lemming_misspeling_stopWords', 'target', forest)\n",
    "print(\"7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\")"
   ],
   "id": "1507303d673732dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 из 7. Расчет точности без препроцессинга завершен\n",
      "2 из 7. Расчет точности после стемминга завершен\n",
      "3 из 7. Расчет точности после лематизации завершен\n",
      "4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\n",
      "5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\n",
      "6 из 7. Расчет точности после удаления стоп-слов завершен\n",
      "7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\n",
      "CPU times: user 1min 33s, sys: 530 ms, total: 1min 33s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:09:15.339788Z",
     "start_time": "2024-06-19T10:09:15.335116Z"
    }
   },
   "cell_type": "code",
   "source": "results_forest",
   "id": "83c1360ccd1e79ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             preprocces  0 or 1 if the word exists  \\\n",
       "0                     just tokenization                   0.904393   \n",
       "1                              stemming                   0.903101   \n",
       "2                         lemmatization                   0.900517   \n",
       "3                 stemming+misspellings                   0.905685   \n",
       "4            lemmatization+misspellings                   0.905685   \n",
       "5                             stopWords                   0.870801   \n",
       "6  lemmatization+misspellings+stopWords                   0.874677   \n",
       "\n",
       "   word counts     TFIDF  Word2Vec  \n",
       "0     0.879845  0.874677  0.864341  \n",
       "1     0.882429  0.881137  0.859173  \n",
       "2     0.883721  0.879845  0.848837  \n",
       "3     0.878553  0.881137  0.846253  \n",
       "4     0.877261  0.873385  0.847545  \n",
       "5     0.869509  0.873385  0.866925  \n",
       "6     0.870801  0.868217  0.865633  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocces</th>\n",
       "      <th>0 or 1 if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just tokenization</td>\n",
       "      <td>0.904393</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>0.874677</td>\n",
       "      <td>0.864341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemming</td>\n",
       "      <td>0.903101</td>\n",
       "      <td>0.882429</td>\n",
       "      <td>0.881137</td>\n",
       "      <td>0.859173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatization</td>\n",
       "      <td>0.900517</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>0.848837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stemming+misspellings</td>\n",
       "      <td>0.905685</td>\n",
       "      <td>0.878553</td>\n",
       "      <td>0.881137</td>\n",
       "      <td>0.846253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatization+misspellings</td>\n",
       "      <td>0.905685</td>\n",
       "      <td>0.877261</td>\n",
       "      <td>0.873385</td>\n",
       "      <td>0.847545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopWords</td>\n",
       "      <td>0.870801</td>\n",
       "      <td>0.869509</td>\n",
       "      <td>0.873385</td>\n",
       "      <td>0.866925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatization+misspellings+stopWords</td>\n",
       "      <td>0.874677</td>\n",
       "      <td>0.870801</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>0.865633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4. Метод опорных векторов",
   "id": "4e81a74ca330603c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:11:21.996947Z",
     "start_time": "2024-06-19T10:09:15.340439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "svc = SVC(C=1,\n",
    "          class_weight=None,\n",
    "          gamma='scale',\n",
    "          kernel='linear',\n",
    "          random_state=21)\n",
    "\n",
    "# Создаем датафрейм кудла будем заносить метрики\n",
    "results_svc = pd.DataFrame({'preprocces':[], '0 or 1 if the word exists': [], 'word counts':[], 'TFIDF':[], 'Word2Vec':[]})\n",
    "\n",
    "# Заполняем табличку\n",
    "results_svc.loc[len(results_svc)] = make_classification('just tokenization', df_temp, 'tokens', 'target', svc)\n",
    "print(\"1 из 7. Расчет точности без препроцессинга завершен\")\n",
    "\n",
    "results_svc.loc[len(results_svc)] = make_classification('stemming', df_temp, 'stemming', 'target', svc)\n",
    "print(\"2 из 7. Расчет точности после стемминга завершен\")\n",
    "\n",
    "results_svc.loc[len(results_svc)] = make_classification('lemmatization', df_temp, 'lemming', 'target', svc)\n",
    "print(\"3 из 7. Расчет точности после лематизации завершен\")\n",
    "\n",
    "results_svc.loc[len(results_svc)] = make_classification('stemming+misspellings', df_temp, 'stemming_misspeling', 'target', svc)\n",
    "print(\"4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\")\n",
    "\n",
    "results_svc.loc[len(results_svc)] = make_classification('lemmatization+misspellings', df_temp, 'lemming_misspeling', 'target', svc)\n",
    "print(\"5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\")\n",
    "\n",
    "results_svc.loc[len(results_svc)] = make_classification('stopWords', df_temp, 'stopWords', 'target', svc)\n",
    "print(\"6 из 7. Расчет точности после удаления стоп-слов завершен\")\n",
    "\n",
    "results_svc.loc[len(results_svc)] = make_classification('lemmatization+misspellings+stopWords', df_temp, 'lemming_misspeling_stopWords', 'target', svc)\n",
    "print(\"7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\")"
   ],
   "id": "9052fe662234667c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 из 7. Расчет точности без препроцессинга завершен\n",
      "2 из 7. Расчет точности после стемминга завершен\n",
      "3 из 7. Расчет точности после лематизации завершен\n",
      "4 из 7. Расчет точности после исправления орфографических ошибок и стеминга завершен\n",
      "5 из 7. Расчет точности после исправления орфографических ошибок и лематизации завершен\n",
      "6 из 7. Расчет точности после удаления стоп-слов завершен\n",
      "7 из 7. Расчет точности после лематизации, исправления ошибок и удаления стоп-слов завершен\n",
      "CPU times: user 2min 10s, sys: 946 ms, total: 2min 11s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:11:22.002447Z",
     "start_time": "2024-06-19T10:11:21.997840Z"
    }
   },
   "cell_type": "code",
   "source": "results_svc",
   "id": "514c209e78ac9bb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             preprocces  0 or 1 if the word exists  \\\n",
       "0                     just tokenization                   0.897933   \n",
       "1                              stemming                   0.901809   \n",
       "2                         lemmatization                   0.897933   \n",
       "3                 stemming+misspellings                   0.895349   \n",
       "4            lemmatization+misspellings                   0.900517   \n",
       "5                             stopWords                   0.873385   \n",
       "6  lemmatization+misspellings+stopWords                   0.878553   \n",
       "\n",
       "   word counts     TFIDF  Word2Vec  \n",
       "0     0.883721  0.883721  0.870801  \n",
       "1     0.888889  0.891473  0.856589  \n",
       "2     0.881137  0.888889  0.864341  \n",
       "3     0.881137  0.887597  0.864341  \n",
       "4     0.882429  0.887597  0.869509  \n",
       "5     0.882429  0.887597  0.868217  \n",
       "6     0.878553  0.879845  0.857881  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocces</th>\n",
       "      <th>0 or 1 if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just tokenization</td>\n",
       "      <td>0.897933</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.870801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemming</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.856589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatization</td>\n",
       "      <td>0.897933</td>\n",
       "      <td>0.881137</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.864341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stemming+misspellings</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.881137</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.864341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatization+misspellings</td>\n",
       "      <td>0.900517</td>\n",
       "      <td>0.882429</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.869509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stopWords</td>\n",
       "      <td>0.873385</td>\n",
       "      <td>0.882429</td>\n",
       "      <td>0.887597</td>\n",
       "      <td>0.868217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatization+misspellings+stopWords</td>\n",
       "      <td>0.878553</td>\n",
       "      <td>0.878553</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>0.857881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Выводы",
   "id": "a49ded4e7705817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Лучшие результаты для алгоритмов машиного обучения:\n",
    "\n",
    "| алгоритм обучения | лучшая точность | параметры |\n",
    "|---------------|-----------------|-----------|\n",
    "| логистическая регрессия | 0.9109 | только токенизация и OneHot-вектор |\n",
    "| дерево решений | 0.8941 | только токенизация и OneHot-вектор |\n",
    "| случайный лес | 0.9056 | лематизация+misspeling и OneHot-вектор |\n",
    "| метод опорных векторов | 0.9019 | стеминг и OneHot-вектор |"
   ],
   "id": "a81c20474084ac5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Лучшие результаты для алгоритмов перевода слов в вектора:\n",
    "\n",
    "| алгоритм векторизации                                               | лучшая точность                          | параметры                                      |\n",
    "|---------------------------------------------------------------------|------------------------------------------|------------------------------------------------|\n",
    "| OneHot-вектор | 0.9109                                   | только токенизация и логистическая регрессия   |  \n",
    "| Подсчет слов | 0.8941 | стеминг и логистическая регрессия              |             \n",
    "| TFIDF | 0.8915 | стеминг и метод опорных векторов               | \n",
    "| Word2Vec | 0.8773 | только токенизация и логистичесская регрессия  | "
   ],
   "id": "6aa27b334deed493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b412de04f1b8037c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
