{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLP beginer\n",
    "\n",
    "https://github.com/proger-n/DS_01_Tweets/blob/main/src/tweets.ipynb\n",
    "\n",
    "### Introduction\n",
    "NLP (natural language processing) — это область знаний и ряд методов и алгоритмов, которые помогают обрабатывать текстовые данные и извлекать из них информацию. Вы уже работали со структурированными данными — таблицами, полными непрерывных и категориальных признаков. Текст является примером полуструктурированных данных. С ним нужно что-то делать, чтобы использовать его, например, в задачах машинного обучения.\n",
    "\n",
    "ЦЕЛЬ РАБОТЫ - предварительная обработка тектовых данных и обучение с помощью различных классификаторов для решения задачи классификации твитов на три класса (негативный, отрицательный и нейтральный)"
   ],
   "id": "62251287a709a8e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Предварительная обработка текста:\n",
    "Здесь перед подачей в классификатор необходимо предобработать текст, есть следующие способы:\n",
    "- [Term Frequency-Inverse Document Frequency (TF-IDF)](https://habr.com/ru/companies/otus/articles/755772/) - это один из наиболее распространенных и мощных методов для извлечения признаков из текстовых данных. TF-IDF вычисляет важность каждого слова в документе относительно количества его употреблений в данном документе и во всей коллекции текстов. Этот метод позволяет выделить ключевые слова и понять, какие слова имеют больший вес для определенного документа в контексте всей коллекции.\n",
    "Перед тем как вычислять TF-IDF, мы должны выполнить предварительную обработку, такую как удаление стоп-слов, приведение к нижнему регистру и токенизация — разбиение текстов на отдельные слова или токены.\n",
    "- Стемминг (stemming) - это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов. Например слово \"Коты\", станет \"Кот\"\n",
    "- Лематизация - это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.\n",
    "\n",
    "Примеры лематизации и стемминга: Слово good – это лемма для слова better. Стеммер не увидит эту связь, так как здесь нужно сверяться со словарем.\n",
    "Слово play – это базовая форма слова playing. Тут справятся и стемминг, и лемматизация.\n",
    "Слово meeting может быть как нормальной формой существительного, так и формой глагола to meet, в зависимости от контекста.\n",
    "В отличие от стемминга, лемматизация попробует выбрать правильную лемму,опираясь на контекст."
   ],
   "id": "551ed269b1a41b58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0.Import",
   "id": "8a3bea984b74eff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:49:02.662519Z",
     "start_time": "2024-06-14T12:48:57.021301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from autocorrect import Speller\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "nltk.download('popular')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/nikolai/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Подготовка данных\n",
    "Данные хранятся в архиве. в Архиве три файла csv. В одном файле негативные отзыва, в другом позитивные и в третьем нейтральные. csv файлы содержат одну строку и много столбцов, поэтому нужно будет трансопнировать и привести всё в одну таблицу"
   ],
   "id": "c3084dec204c68f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T10:27:21.480437Z",
     "start_time": "2024-06-13T10:27:21.306678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Распаковываем zip архив\n",
    "with zipfile.ZipFile('../datasets/p00_tweets.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "    \n",
    "# Читаем файлы в pandas\n",
    "positive_tweets = pd.read_csv('data/processedPositive.csv')\n",
    "print(positive_tweets.shape)\n",
    "\n",
    "neutral_tweets = pd.read_csv('data/processedNeutral.csv')\n",
    "print(neutral_tweets.shape)\n",
    "\n",
    "negative_tweets = pd.read_csv('data/processedNegative.csv')\n",
    "print(negative_tweets.shape)"
   ],
   "id": "c07438aadbb8d87e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1186)\n",
      "(0, 1570)\n",
      "(0, 1117)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1. Приведем все данные в одну таблицу.\n",
    "Для этого транспонируем исходные таблицы и назначим индексы\n",
    "- 1 - негативный твит\n",
    "- 2 - нейтральный твит\n",
    "- 3 - положительный твит"
   ],
   "id": "f794af4219eab189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T10:27:24.471057Z",
     "start_time": "2024-06-13T10:27:24.467875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(df, target_index):\n",
    "    df_new = df.T.reset_index().rename(columns={'index': 'tweet'})\n",
    "    df_new['target'] = target_index\n",
    "    return df_new"
   ],
   "id": "3ddfc15396d3770b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T10:27:29.894870Z",
     "start_time": "2024-06-13T10:27:29.878968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "negative_tweets = prepare_data(negative_tweets, target_index=1)\n",
    "neutral_tweets = prepare_data(neutral_tweets, target_index=2)\n",
    "positive_tweets = prepare_data(positive_tweets, target_index=3)\n",
    "\n",
    "df = pd.concat([negative_tweets, neutral_tweets, positive_tweets], ignore_index=True)\n",
    "df"
   ],
   "id": "622dc132b66c68d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  tweet  target\n",
       "0                 How unhappy  some dogs like it though       1\n",
       "1     talking to my over driver about where I'm goin...       1\n",
       "2     Does anybody know if the Rand's likely to fall...       1\n",
       "3            I miss going to gigs in Liverpool unhappy        1\n",
       "4         There isnt a new Riverdale tonight ? unhappy        1\n",
       "...                                                 ...     ...\n",
       "3868  Thanks for the recent follow Happy to connect ...       3\n",
       "3869              - top engaged members this week happy       3\n",
       "3870  ngam to  weeks left for cadet pilot exam cryin...       3\n",
       "3871            Great! You're welcome Josh happy  ^Adam       3\n",
       "3872  Sixth spot not applicable Team! Higher pa! :)K...       3\n",
       "\n",
       "[3873 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>Thanks for the recent follow Happy to connect ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>- top engaged members this week happy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam cryin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Great! You're welcome Josh happy  ^Adam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Sixth spot not applicable Team! Higher pa! :)K...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2. чистим твиты\n",
    "убираем все символы кроме букв и преобразуем к нижнему регистру"
   ],
   "id": "c8773d5415079697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T10:27:39.874132Z",
     "start_time": "2024-06-13T10:27:39.863159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "df = standardize_text(df, 'tweet')\n",
    "\n",
    "df"
   ],
   "id": "657a588af60dfc08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  tweet  target\n",
       "0                 how unhappy  some dogs like it though       1\n",
       "1     talking to my over driver about where im going...       1\n",
       "2     does anybody know if the rands likely to fall ...       1\n",
       "3            i miss going to gigs in liverpool unhappy        1\n",
       "4          there isnt a new riverdale tonight  unhappy        1\n",
       "...                                                 ...     ...\n",
       "3868  thanks for the recent follow happy to connect ...       3\n",
       "3869                top engaged members this week happy       3\n",
       "3870  ngam to  weeks left for cadet pilot exam cryin...       3\n",
       "3871               great youre welcome josh happy  adam       3\n",
       "3872  sixth spot not applicable team higher pa kisse...       3\n",
       "\n",
       "[3873 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where im going...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does anybody know if the rands likely to fall ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there isnt a new riverdale tonight  unhappy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>thanks for the recent follow happy to connect ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>top engaged members this week happy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>ngam to  weeks left for cadet pilot exam cryin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>great youre welcome josh happy  adam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>sixth spot not applicable team higher pa kisse...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3. Токенизация\n",
    "Разбиваем твиты на токены"
   ],
   "id": "22ec3fad41ee711a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T10:43:54.278240Z",
     "start_time": "2024-06-13T10:43:54.264291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df['tokens'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "df.head()"
   ],
   "id": "9dc5be9961185483",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               tweet  target  \\\n",
       "0              how unhappy  some dogs like it though       1   \n",
       "1  talking to my over driver about where im going...       1   \n",
       "2  does anybody know if the rands likely to fall ...       1   \n",
       "3         i miss going to gigs in liverpool unhappy        1   \n",
       "4       there isnt a new riverdale tonight  unhappy        1   \n",
       "\n",
       "                                              tokens  \n",
       "0       [how, unhappy, some, dogs, like, it, though]  \n",
       "1  [talking, to, my, over, driver, about, where, ...  \n",
       "2  [does, anybody, know, if, the, rands, likely, ...  \n",
       "3  [i, miss, going, to, gigs, in, liverpool, unha...  \n",
       "4  [there, isnt, a, new, riverdale, tonight, unha...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how unhappy  some dogs like it though</td>\n",
       "      <td>1</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where im going...</td>\n",
       "      <td>1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does anybody know if the rands likely to fall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[does, anybody, know, if, the, rands, likely, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i miss going to gigs in liverpool unhappy</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there isnt a new riverdale tonight  unhappy</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T11:52:10.142193Z",
     "start_time": "2024-06-13T11:52:10.130111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = [word for tokens in df['tokens'] for word in tokens] # Все слова во всех твитах\n",
    "tweet_length = [len(tokens) for tokens in df['tokens']] # список с количеством слов в твите\n",
    "different_words = sorted(list(set(all_words))) # список различных слов\n",
    "print('Всего слов в твитах: ', len(all_words))\n",
    "print('Длина максимального твита: ', max(tweet_length))\n",
    "print('Длина минимального твита: ', min(tweet_length))\n",
    "print('Различных слов во всех твитах: ', len(different_words))"
   ],
   "id": "cbccd1da7785dae0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов в твитах:  33208\n",
      "Длина максимального твита:  30\n",
      "Длина минимального твита:  0\n",
      "Различных слов во всех твитах:  6378\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4. Функции для трансформорации слова в векторы с использованием различных подходов:\n",
    "- бинаризация, переделка в onehot vector\n",
    "- подсчет слов\n",
    "- TFIDF\n",
    "\n",
    "Все методы векторизации предназначены для перевода слов в вектора (числа)\n",
    "\n",
    "про MultiLabelBinarizer() написано [тут](https://ru.stackoverflow.com/questions/928443/%D0%9A%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-sklearn-preprocessing-multilabelbinarizer), [тут](https://spec-zone.ru/scikit_learn/modules/generated/sklearn.preprocessing.multilabelbinarizer) и [тут](https://scikit-learn.ru/1-12-multiclass-and-multioutput-algorithms/#multiclass-classification). В целом данный класс преобразует столбец с токенами в One-Hot-Encoded\n",
    "\n"
   ],
   "id": "53b999027e51fb4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:24:49.447867Z",
     "start_time": "2024-06-14T13:24:49.444061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_tokens_and_split(df, tokens_column_name, target_column_name, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Преобразует матрицу с токенами в one-hot-векторы и разбиват на тренировочную и тестовую\n",
    "    :param df: датафрейм с токенами и целевой переменной\n",
    "    :param tokens_column_name: название столбца датафрейма с токенами\n",
    "    :param target_column_name: название столбца датафрейма с целевой переменной\n",
    "    :param test_size: размер тестовой выборки\n",
    "    :return: X_train_bin, X_test_bin, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[tokens_column_name],\n",
    "                                                        df[target_column_name],\n",
    "                                                        stratify=df[target_column_name],\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(X_train)  # получает классы - один класс - одно слово, где слова не повторяются\n",
    "    X_train_bin = mlb.transform(X_train) # преобразуем в onehot vector\n",
    "    X_test_bin = mlb.transform(X_test)  # преобразуем в onehot vector\n",
    "    return X_train_bin, X_test_bin, y_train, y_test"
   ],
   "id": "9c5d2084ec6bc14b",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "про CountVectorizer() можно ознакомится [тута](https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/vectorizers_tutorial_mvsamsonov.ipynb), [тута](https://habr.com/ru/articles/702626/) и [тут](https://habr.com/ru/articles/205360/). \n",
    "Если коротко то данный класс преобразует твиты в матрицу значениями которой, являются количества вхождения данного ключа(слова) в текст. Следует отметить что здесь нужно передать не список токенов, а список предложений. По умолчанию возвращается разреженная матрица"
   ],
   "id": "d03c1b6938729cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:36:01.858852Z",
     "start_time": "2024-06-14T13:36:01.854559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_words_and_split(df, tokens_column_name, target_column_name, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Преобразует матрицу с токенами сначала в матрице предложений,\n",
    "    Затем заносит в матрицу для каждого слова - сколько раз оно встречалось во всей выборке\n",
    "    Возвращает разреженную матрицу\n",
    "    \n",
    "    :param df: датафрейм с токенами и целевой переменной\n",
    "    :param tokens_column_name: название столбца датафрейма с токенами\n",
    "    :param target_column_name: название столбца датафрейма с целевой переменной\n",
    "    :param test_size: размер тестовой выборки\n",
    "    :return: X_train_counts, X_test_counts, y_train, y_test\n",
    "    \"\"\"\n",
    "    list_corpus = df[tokens_column_name].apply(lambda x: ' '.join(x)).tolist() # из токенов получаем как бы список твитов в одном предложении\n",
    "    list_labels = df[target_column_name].tolist() # преобразуем целевые переменные в список\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, stratify=list_labels, test_size=test_size, random_state=21)\n",
    "    \n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train_counts = count_vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test_counts = count_vectorizer.transform(X_test).toarray()\n",
    "    \n",
    "    return X_train_counts, X_test_counts, y_train, y_test\n",
    "\n",
    "    "
   ],
   "id": "68d6432e2021a520",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "про TFIDF написано [тут](https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/vectorizers_tutorial_mvsamsonov.ipynb)\n",
    "Если слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения CountVectorizer различий не будет. Здесь на помощь приходит TFIDF.\n",
    "\n",
    "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки."
   ],
   "id": "8a9473b4abdfcde1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:58:39.127264Z",
     "start_time": "2024-06-14T09:58:39.124460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tfidf_and_split(df, tokens_column_name, target_column_name, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Преобазует матрицу с токенами в TFIDF\n",
    "    \n",
    "    :param df: датафрейм с токенами и целевой переменной\n",
    "    :param tokens_column_name: название столбца датафрейма с токенами\n",
    "    :param target_column_name: название столбца датафрейма с целевой переменной\n",
    "    :param test_size: размер тестовой выборки\n",
    "    :return: X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "    \"\"\"\n",
    "    list_corpus = df[tokens_column_name].apply(lambda x: ' '.join(x)).tolist()\n",
    "    list_labels = df[target_column_name].tolist()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, stratify=list_labels, test_size=test_size, random_state=21)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(X_train)\n",
    "    X_train_tfidf = tfidf.transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test"
   ],
   "id": "1db60f1726f18893",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5. Функции для препроцессинга слов перед векторизацией",
   "id": "cef0fba3feeec336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:58:42.754230Z",
     "start_time": "2024-06-14T09:58:42.751645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemming(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    Применяет лематизацию к токенам\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца к которому нужно применить лематизацию\n",
    "    :return: столбец с привденными леммами\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return df[tokens_column_name].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ],
   "id": "c2ac761be28a8777",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:58:43.006734Z",
     "start_time": "2024-06-14T09:58:43.003643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stemming(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    Применяет стемминг к токенам\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца к которому нужно применить стемминг\n",
    "    :return: столбец с привденными формами слов\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return df[tokens_column_name].apply(lambda x: [stemmer.stem(word) for word in x])"
   ],
   "id": "134b0d42af220138",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:58:43.848987Z",
     "start_time": "2024-06-14T09:58:43.845164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stop_words_remove(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    удаляет стоп слова\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца в котором нужно удалить стоп слова\n",
    "    :return: столбец с удаленными стоп словами\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return df[tokens_column_name].apply(lambda x: [word for word in x if word not in stop_words])"
   ],
   "id": "d93b91ce57e219a2",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:58:46.398722Z",
     "start_time": "2024-06-14T09:58:46.396013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def misspelling(df, tokens_column_name):\n",
    "    \"\"\"\n",
    "    Исправляет орфографические ошибки в словах\n",
    "    :param df: датафрейм\n",
    "    :param tokens_column_name:  название столбца в котором нужно исправить орфографические ошибки\n",
    "    :return: столбец с исправлеными токенами\n",
    "    \"\"\"\n",
    "    speller = Speller(lang=\"en\")\n",
    "    return df[tokens_column_name].apply(lambda words: [speller(word) for word in words])"
   ],
   "id": "668b475d104e4824",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.6. Вспомогатльная функция для заполнения таблицы",
   "id": "b3ebb61408457588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T09:58:51.395347Z",
     "start_time": "2024-06-14T09:58:51.392188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_classification(prep_name, df, tokens_column, target_column, model):\n",
    "    result = [prep_name]\n",
    "    X_train, X_test, y_train, y_test = binarize_tokens_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = count_words_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = tfidf_and_split(df, tokens_column, target_column)\n",
    "    result.append(accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)))\n",
    "\n",
    "    return result"
   ],
   "id": "dee8bf5d5c7c3a7d",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. ML-алгоритмы\n",
    "Поскольк нам дали возможность выбирать модели самим, будем пробовать четыре модели:\n",
    "- логистическая регрессия\n",
    "- дерево решений\n",
    "- случайный лес\n",
    "- метод опорных векторов\n",
    "\n",
    "Так же попробуем использовать GridSeach для выбора оптиального параметра для каждо модели.\n",
    "Поскольку есть много вариаций для которой можно применить GreadSearch, мы сначала применим его для TFIDF с предварительным препроцессингом датасета (стеминг лематизация и удаления стоп слов)\n",
    "Затем выявим лучшие параметры и уже модель с этими параметрами применим ко всем вариациям согласно заданию и заполним таблицы для каждой модели."
   ],
   "id": "b2dd8155a209382a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.0. Для наглядности как работают лематизация, стемминг, исправление ошибок и удаление стоп слов - создадим новый датафрейм\n",
    "Посмотрим как выглядит столбец с токенами после каждой из данных процедур"
   ],
   "id": "ad3571f98c21f710"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:10:52.535662Z",
     "start_time": "2024-06-14T11:10:13.790009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# сначала попробуем выбрать лучшие параметры получные для тренировочной выборки с использованием GridSearch\n",
    "df_temp = df.copy()\n",
    "df_temp.drop('tweet', axis=1, inplace=True)\n",
    "df_temp['tokens_spell'] = misspelling(df_temp, 'tokens') # сначал справим ошибки в словах\n",
    "df_temp['tokens_s_lemma'] = lemming(df_temp, 'tokens_spell') # потом применим лематизацию\n",
    "df_temp['tokens_s_l_stem'] = stemming(df_temp, 'tokens_s_lemma') # потом стемминг\n",
    "df_temp['tokens_total'] = stemming(df_temp, 'tokens_s_l_stem') # и затем удалим стоп-слова\n",
    "df_temp"
   ],
   "id": "7de3b736b863bb1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      target  \\\n",
       "0          1   \n",
       "1          1   \n",
       "2          1   \n",
       "3          1   \n",
       "4          1   \n",
       "...      ...   \n",
       "3868       3   \n",
       "3869       3   \n",
       "3870       3   \n",
       "3871       3   \n",
       "3872       3   \n",
       "\n",
       "                                                                                                                                                                            tokens  \\\n",
       "0                                                                                                                                     [how, unhappy, some, dogs, like, it, though]   \n",
       "1                                   [talking, to, my, over, driver, about, where, im, goinghe, said, hed, love, to, go, to, new, york, too, but, since, trump, its, probably, not]   \n",
       "2     [does, anybody, know, if, the, rands, likely, to, fall, against, the, dollar, i, got, some, money, i, need, to, change, into, r, but, it, keeps, getting, stronger, unhappy]   \n",
       "3                                                                                                                               [i, miss, going, to, gigs, in, liverpool, unhappy]   \n",
       "4                                                                                                                               [there, isnt, a, new, riverdale, tonight, unhappy]   \n",
       "...                                                                                                                                                                            ...   \n",
       "3868                                                                            [thanks, for, the, recent, follow, happy, to, connect, happy, have, a, great, thursday, get, this]   \n",
       "3869                                                                                                                                    [top, engaged, members, this, week, happy]   \n",
       "3870                                                                                                           [ngam, to, weeks, left, for, cadet, pilot, exam, crying, with, joy]   \n",
       "3871                                                                                                                                    [great, youre, welcome, josh, happy, adam]   \n",
       "3872                                                                                                      [sixth, spot, not, applicable, team, higher, pa, kisses, thefashionicon]   \n",
       "\n",
       "                                                                                                                                                                      tokens_spell  \\\n",
       "0                                                                                                                                     [how, unhappy, some, dogs, like, it, though]   \n",
       "1                                     [talking, to, my, over, driver, about, where, im, going, said, hed, love, to, go, to, new, york, too, but, since, trump, its, probably, not]   \n",
       "2     [does, anybody, know, if, the, hands, likely, to, fall, against, the, dollar, i, got, some, money, i, need, to, change, into, r, but, it, keeps, getting, stronger, unhappy]   \n",
       "3                                                                                                                               [i, miss, going, to, gigs, in, liverpool, unhappy]   \n",
       "4                                                                                                                               [there, isnt, a, new, riverdale, tonight, unhappy]   \n",
       "...                                                                                                                                                                            ...   \n",
       "3868                                                                            [thanks, for, the, recent, follow, happy, to, connect, happy, have, a, great, thursday, get, this]   \n",
       "3869                                                                                                                                    [top, engaged, members, this, week, happy]   \n",
       "3870                                                                                                            [nam, to, weeks, left, for, cadet, pilot, exam, crying, with, joy]   \n",
       "3871                                                                                                                                    [great, youre, welcome, josh, happy, adam]   \n",
       "3872                                                                                                      [sixth, spot, not, applicable, team, higher, pa, kisses, thefashionicon]   \n",
       "\n",
       "                                                                                                                                                                 tokens_s_lemma  \\\n",
       "0                                                                                                                                   [how, unhappy, some, dog, like, it, though]   \n",
       "1                                   [talking, to, my, over, driver, about, where, im, going, said, hed, love, to, go, to, new, york, too, but, since, trump, it, probably, not]   \n",
       "2     [doe, anybody, know, if, the, hand, likely, to, fall, against, the, dollar, i, got, some, money, i, need, to, change, into, r, but, it, keep, getting, stronger, unhappy]   \n",
       "3                                                                                                                             [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4                                                                                                                            [there, isnt, a, new, riverdale, tonight, unhappy]   \n",
       "...                                                                                                                                                                         ...   \n",
       "3868                                                                         [thanks, for, the, recent, follow, happy, to, connect, happy, have, a, great, thursday, get, this]   \n",
       "3869                                                                                                                                  [top, engaged, member, this, week, happy]   \n",
       "3870                                                                                                             [nam, to, week, left, for, cadet, pilot, exam, cry, with, joy]   \n",
       "3871                                                                                                                                 [great, youre, welcome, josh, happy, adam]   \n",
       "3872                                                                                                     [sixth, spot, not, applicable, team, higher, pa, kiss, thefashionicon]   \n",
       "\n",
       "                                                                                                                                                         tokens_s_l_stem  \\\n",
       "0                                                                                                                            [how, unhappi, some, dog, like, it, though]   \n",
       "1                                    [talk, to, my, over, driver, about, where, im, go, said, hed, love, to, go, to, new, york, too, but, sinc, trump, it, probabl, not]   \n",
       "2     [doe, anybodi, know, if, the, hand, like, to, fall, against, the, dollar, i, got, some, money, i, need, to, chang, into, r, but, it, keep, get, stronger, unhappi]   \n",
       "3                                                                                                                         [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4                                                                                                                      [there, isnt, a, new, riverdal, tonight, unhappi]   \n",
       "...                                                                                                                                                                  ...   \n",
       "3868                                                                    [thank, for, the, recent, follow, happi, to, connect, happi, have, a, great, thursday, get, thi]   \n",
       "3869                                                                                                                              [top, engag, member, thi, week, happi]   \n",
       "3870                                                                                                      [nam, to, week, left, for, cadet, pilot, exam, cri, with, joy]   \n",
       "3871                                                                                                                            [great, your, welcom, josh, happi, adam]   \n",
       "3872                                                                                                  [sixth, spot, not, applic, team, higher, pa, kiss, thefashionicon]   \n",
       "\n",
       "                                                                                                                                                            tokens_total  \n",
       "0                                                                                                                            [how, unhappi, some, dog, like, it, though]  \n",
       "1                                    [talk, to, my, over, driver, about, where, im, go, said, hed, love, to, go, to, new, york, too, but, sinc, trump, it, probabl, not]  \n",
       "2     [doe, anybodi, know, if, the, hand, like, to, fall, against, the, dollar, i, got, some, money, i, need, to, chang, into, r, but, it, keep, get, stronger, unhappi]  \n",
       "3                                                                                                                         [i, miss, go, to, gig, in, liverpool, unhappi]  \n",
       "4                                                                                                                        [there, isnt, a, new, riverd, tonight, unhappi]  \n",
       "...                                                                                                                                                                  ...  \n",
       "3868                                                                    [thank, for, the, recent, follow, happi, to, connect, happi, have, a, great, thursday, get, thi]  \n",
       "3869                                                                                                                              [top, engag, member, thi, week, happi]  \n",
       "3870                                                                                                      [nam, to, week, left, for, cadet, pilot, exam, cri, with, joy]  \n",
       "3871                                                                                                                            [great, your, welcom, josh, happi, adam]  \n",
       "3872                                                                                                  [sixth, spot, not, applic, team, higher, pa, kiss, thefashionicon]  \n",
       "\n",
       "[3873 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_spell</th>\n",
       "      <th>tokens_s_lemma</th>\n",
       "      <th>tokens_s_l_stem</th>\n",
       "      <th>tokens_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, im, goinghe, said, hed, love, to, go, to, new, york, too, but, since, trump, its, probably, not]</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, im, going, said, hed, love, to, go, to, new, york, too, but, since, trump, its, probably, not]</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, im, going, said, hed, love, to, go, to, new, york, too, but, since, trump, it, probably, not]</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, im, go, said, hed, love, to, go, to, new, york, too, but, sinc, trump, it, probabl, not]</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, im, go, said, hed, love, to, go, to, new, york, too, but, sinc, trump, it, probabl, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[does, anybody, know, if, the, rands, likely, to, fall, against, the, dollar, i, got, some, money, i, need, to, change, into, r, but, it, keeps, getting, stronger, unhappy]</td>\n",
       "      <td>[does, anybody, know, if, the, hands, likely, to, fall, against, the, dollar, i, got, some, money, i, need, to, change, into, r, but, it, keeps, getting, stronger, unhappy]</td>\n",
       "      <td>[doe, anybody, know, if, the, hand, likely, to, fall, against, the, dollar, i, got, some, money, i, need, to, change, into, r, but, it, keep, getting, stronger, unhappy]</td>\n",
       "      <td>[doe, anybodi, know, if, the, hand, like, to, fall, against, the, dollar, i, got, some, money, i, need, to, chang, into, r, but, it, keep, get, stronger, unhappi]</td>\n",
       "      <td>[doe, anybodi, know, if, the, hand, like, to, fall, against, the, dollar, i, got, some, money, i, need, to, chang, into, r, but, it, keep, get, stronger, unhappi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unhappy]</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unhappy]</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, unhappy]</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, unhappi]</td>\n",
       "      <td>[there, isnt, a, new, riverd, tonight, unhappi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>3</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, connect, happy, have, a, great, thursday, get, this]</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, connect, happy, have, a, great, thursday, get, this]</td>\n",
       "      <td>[thanks, for, the, recent, follow, happy, to, connect, happy, have, a, great, thursday, get, this]</td>\n",
       "      <td>[thank, for, the, recent, follow, happi, to, connect, happi, have, a, great, thursday, get, thi]</td>\n",
       "      <td>[thank, for, the, recent, follow, happi, to, connect, happi, have, a, great, thursday, get, thi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>3</td>\n",
       "      <td>[top, engaged, members, this, week, happy]</td>\n",
       "      <td>[top, engaged, members, this, week, happy]</td>\n",
       "      <td>[top, engaged, member, this, week, happy]</td>\n",
       "      <td>[top, engag, member, thi, week, happi]</td>\n",
       "      <td>[top, engag, member, thi, week, happi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>3</td>\n",
       "      <td>[ngam, to, weeks, left, for, cadet, pilot, exam, crying, with, joy]</td>\n",
       "      <td>[nam, to, weeks, left, for, cadet, pilot, exam, crying, with, joy]</td>\n",
       "      <td>[nam, to, week, left, for, cadet, pilot, exam, cry, with, joy]</td>\n",
       "      <td>[nam, to, week, left, for, cadet, pilot, exam, cri, with, joy]</td>\n",
       "      <td>[nam, to, week, left, for, cadet, pilot, exam, cri, with, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>3</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, youre, welcome, josh, happy, adam]</td>\n",
       "      <td>[great, your, welcom, josh, happi, adam]</td>\n",
       "      <td>[great, your, welcom, josh, happi, adam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>3</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, pa, kisses, thefashionicon]</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, pa, kisses, thefashionicon]</td>\n",
       "      <td>[sixth, spot, not, applicable, team, higher, pa, kiss, thefashionicon]</td>\n",
       "      <td>[sixth, spot, not, applic, team, higher, pa, kiss, thefashionicon]</td>\n",
       "      <td>[sixth, spot, not, applic, team, higher, pa, kiss, thefashionicon]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Затем преобразуем конечные токены в TFIDF и разобъем на тренировочный и тестовый датасеты",
   "id": "79c2d1ff8a888298"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:19.941285Z",
     "start_time": "2024-06-14T11:17:19.897689Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_tf, X_test_tf, y_train, y_test = tfidf_and_split(df_temp, target_column_name='target', tokens_column_name='tokens_total')",
   "id": "4422d2d2e7ad01d3",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.0.1. Для начала предварительно выберем \"наилучшие\" параметры для каждой модели",
   "id": "e9a4b6e7d2773166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:20:18.417104Z",
     "start_time": "2024-06-14T13:19:33.783248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ\n",
    "clf = LogisticRegression()\n",
    "param_grid = {'C': [0.1, 1, 10, 30, 50],\n",
    "              'solver': ['newton-cg', 'sag', 'saga'],\n",
    "              'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "              'max_iter': [5000],\n",
    "              'random_state': [21],\n",
    "              'class_weight': ['balanced'],\n",
    "              'multi_class': ['multinomial'],\n",
    "              'n_jobs': [-1]\n",
    "              }\n",
    "gs = GridSearchCV(clf, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "874977c210a03c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'C': 10, 'class_weight': 'balanced', 'max_iter': 5000, 'multi_class': 'multinomial', 'n_jobs': -1, 'penalty': 'l2', 'random_state': 21, 'solver': 'saga'}\n",
      "лучший score: 0.8837881077700767\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:02:05.714640Z",
     "start_time": "2024-06-14T13:02:00.283650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. ДЕРЕВО РЕШЕНИЙ\n",
    "tree = DecisionTreeClassifier()\n",
    "param_grid = {'criterion': ['gini','entropy'],\n",
    "              'max_depth': np.arange(1, 50),\n",
    "              'class_weight': ['balanced', None],\n",
    "              'random_state': [21]\n",
    "            }\n",
    "gs = GridSearchCV(tree, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "e3739224689066c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'class_weight': None, 'criterion': 'gini', 'max_depth': 25, 'random_state': 21}\n",
      "лучший score: 0.8618453280525301\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:09:11.165523Z",
     "start_time": "2024-06-14T13:02:33.900913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Случайный лес\n",
    "forest = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [5, 10, 50, 100, 200, 300],\n",
    "              'criterion': ['gini','entropy'],\n",
    "              'max_depth': np.arange(1, 50),\n",
    "              'class_weight': ['balanced', None],\n",
    "              'random_state': [21]\n",
    "              }\n",
    " \n",
    "gs = GridSearchCV(forest, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "b1df8f24ed221285",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 44, 'n_estimators': 300, 'random_state': 21}\n",
      "лучший score: 0.883148157798739\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:11:51.439684Z",
     "start_time": "2024-06-14T13:11:31.628544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "svc = SVC()\n",
    "param_grid = {'C': [0.01, 0.1, 1, 1.5, 5, 10],\n",
    "              'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "              'gamma': ['scale', 'auto'],\n",
    "              'class_weight': ['balanced', None]\n",
    "              }\n",
    "gs = GridSearchCV(svc, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(f'лучшие параметры: {gs.best_params_}')\n",
    "print(f'лучший score: {gs.best_score_}')"
   ],
   "id": "21776de3e182d5d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лучшие параметры: {'C': 1, 'class_weight': None, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "лучший score: 0.8860498202094951\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Теперь когда мы получили примерно оптимальные гипперпараметры моежм приступить к следующему\n",
    "В следующем шаге мы будем использовать именно \"лучшие\" параметры для каждой модели"
   ],
   "id": "7f76e90436addcd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Логистическая регрессия",
   "id": "62bf5de25abe41ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:50:55.251924Z",
     "start_time": "2024-06-14T13:40:46.273270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "clf = LogisticRegression(C=10,\n",
    "                         class_weight='balanced',\n",
    "                         max_iter=5000,\n",
    "                         solver='saga',\n",
    "                         multi_class='multinomial',\n",
    "                         n_jobs=-1,\n",
    "                         random_state=21,\n",
    "                         penalty='l2',)\n",
    "\n",
    "# Создаем датафрейм кудла будем заносить метрики\n",
    "results_logreg = pd.DataFrame({'preprocces':[], '0 or 1 if the word exists': [], 'word counts':[], 'TFIDF':[]})\n",
    "\n",
    "# Заполняем табличку\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('just tok', df, 'tokens', 'target', clf)\n",
    "print(\"1 из 6. Расчет точности без препроцессинга завершен\")\n",
    "\n",
    "df['stemmed'] = stemming(df, 'tokens')\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('stemming', df, 'stemmed', 'target', clf)\n",
    "print(\"2 из 6. Расчет точности после стемминга завершен\")\n",
    "\n",
    "df['lemmed'] = lemming(df, 'tokens')\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('lemming', df, 'lemmed', 'target', clf)\n",
    "print(\"3 из 6. Расчет точности после лематизации завершен\")\n",
    "\n",
    "df['misspelled'] = misspelling(df, 'tokens')\n",
    "df['misspelled+stemmed'] = stemming(df, 'misspelled')\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('misspelled+stemmed', df, 'misspelled+stemmed', 'target', clf)\n",
    "print(\"4 из 6. Расчет точности после исправления орфографических ошибок и стеминга завершен\")\n",
    "\n",
    "df['misspelled'] = misspelling(df, 'tokens')\n",
    "df['misspelled+lemmed'] = lemming(df, 'misspelled')\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('misspelled+lemmed', df, 'misspelled+lemmed', 'target', clf)\n",
    "print(\"5 из 6. Расчет точности после исправления орфографических ошибок и лематизации завершен\")\n",
    "\n",
    "df['stop_words_removed'] = stop_words_remove(df, 'tokens')\n",
    "results_logreg.loc[len(results_logreg)] = make_classification('stop_words_remove', df, 'stop_words_removed', 'target', clf)\n",
    "print(\"6 из 6. Расчет точности после удаления стоп-слов завершен\")"
   ],
   "id": "75deba7f9bed2e23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 из 6. Расчет точности без препроцессинга завершен\n",
      "2 из 6. Расчет точности после стемминга завершен\n",
      "3 из 6. Расчет точности после лематизации завершен\n",
      "4 из 6. Расчет точности после исправления орфографических ошибок и стеминга завершен\n",
      "5 из 6. Расчет точности после исправления орфографических ошибок и лематизации завершен\n",
      "6 из 6. Расчет точности после удаления стоп-слов завершен\n",
      "CPU times: user 10min 12s, sys: 11.9 s, total: 10min 23s\n",
      "Wall time: 10min 8s\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:51:18.249077Z",
     "start_time": "2024-06-14T13:51:18.243872Z"
    }
   },
   "cell_type": "code",
   "source": "results_logreg",
   "id": "af4dbeb6146cd960",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           preprocces  0 or 1 if the word exists  word counts     TFIDF\n",
       "0            just tok                   0.908387     0.885161  0.887742\n",
       "1            stemming                   0.910968     0.889032  0.894194\n",
       "2             lemming                   0.908387     0.885161  0.891613\n",
       "3  misspelled+stemmed                   0.912258     0.883871  0.896774\n",
       "4   misspelled+lemmed                   0.909677     0.883871  0.896774\n",
       "5   stop_words_remove                   0.889032     0.890323  0.894194"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocces</th>\n",
       "      <th>0 or 1 if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just tok</td>\n",
       "      <td>0.908387</td>\n",
       "      <td>0.885161</td>\n",
       "      <td>0.887742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemming</td>\n",
       "      <td>0.910968</td>\n",
       "      <td>0.889032</td>\n",
       "      <td>0.894194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemming</td>\n",
       "      <td>0.908387</td>\n",
       "      <td>0.885161</td>\n",
       "      <td>0.891613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>misspelled+stemmed</td>\n",
       "      <td>0.912258</td>\n",
       "      <td>0.883871</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>misspelled+lemmed</td>\n",
       "      <td>0.909677</td>\n",
       "      <td>0.883871</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stop_words_remove</td>\n",
       "      <td>0.889032</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.894194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Дерево решений",
   "id": "505858d5add9e756"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:52:58.948197Z",
     "start_time": "2024-06-14T13:51:34.506702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "tree = DecisionTreeClassifier(class_weight=None,\n",
    "                              criterion='gini',\n",
    "                              max_depth=25,\n",
    "                              random_state=21)\n",
    "\n",
    "# Создаем датафрейм кудла будем заносить метрики\n",
    "results_tree = pd.DataFrame({'preprocces':[], '0 or 1 if the word exists': [], 'word counts':[], 'TFIDF':[]})\n",
    "\n",
    "# Заполняем табличку\n",
    "results_tree.loc[len(results_tree)] = make_classification('just tok', df, 'tokens', 'target', tree)\n",
    "print(\"1 из 6. Расчет точности без препроцессинга завершен\")\n",
    "\n",
    "df['stemmed'] = stemming(df, 'tokens')\n",
    "results_tree.loc[len(results_tree)] = make_classification('stemming', df, 'stemmed', 'target', tree)\n",
    "print(\"2 из 6. Расчет точности после стемминга завершен\")\n",
    "\n",
    "df['lemmed'] = lemming(df, 'tokens')\n",
    "results_tree.loc[len(results_tree)] = make_classification('lemming', df, 'lemmed', 'target', tree)\n",
    "print(\"3 из 6. Расчет точности после лематизации завершен\")\n",
    "\n",
    "df['misspelled'] = misspelling(df, 'tokens')\n",
    "df['misspelled+stemmed'] = stemming(df, 'misspelled')\n",
    "results_tree.loc[len(results_tree)] = make_classification('misspelled+stemmed', df, 'misspelled+stemmed', 'target', tree)\n",
    "print(\"4 из 6. Расчет точности после исправления орфографических ошибок и стеминга завершен\")\n",
    "\n",
    "df['misspelled'] = misspelling(df, 'tokens')\n",
    "df['misspelled+lemmed'] = lemming(df, 'misspelled')\n",
    "results_tree.loc[len(results_tree)] = make_classification('misspelled+lemmed', df, 'misspelled+lemmed', 'target', tree)\n",
    "print(\"5 из 6. Расчет точности после исправления орфографических ошибок и лематизации завершен\")\n",
    "\n",
    "df['stop_words_removed'] = stop_words_remove(df, 'tokens')\n",
    "results_tree.loc[len(results_tree)] = make_classification('stop_words_remove', df, 'stop_words_removed', 'target', tree)\n",
    "print(\"6 из 6. Расчет точности после удаления стоп-слов завершен\")"
   ],
   "id": "885559e3438213b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 из 6. Расчет точности без препроцессинга завершен\n",
      "2 из 6. Расчет точности после стемминга завершен\n",
      "3 из 6. Расчет точности после лематизации завершен\n",
      "4 из 6. Расчет точности после исправления орфографических ошибок и стеминга завершен\n",
      "5 из 6. Расчет точности после исправления орфографических ошибок и лематизации завершен\n",
      "6 из 6. Расчет точности после удаления стоп-слов завершен\n",
      "CPU times: user 1min 24s, sys: 338 ms, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:52:58.953468Z",
     "start_time": "2024-06-14T13:52:58.949090Z"
    }
   },
   "cell_type": "code",
   "source": "results_tree",
   "id": "cfd68c36858471a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           preprocces  0 or 1 if the word exists  word counts     TFIDF\n",
       "0            just tok                   0.898065     0.878710  0.868387\n",
       "1            stemming                   0.886452     0.873548  0.867097\n",
       "2             lemming                   0.895484     0.876129  0.861935\n",
       "3  misspelled+stemmed                   0.891613     0.863226  0.876129\n",
       "4   misspelled+lemmed                   0.895484     0.876129  0.876129\n",
       "5   stop_words_remove                   0.869677     0.869677  0.868387"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocces</th>\n",
       "      <th>0 or 1 if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just tok</td>\n",
       "      <td>0.898065</td>\n",
       "      <td>0.878710</td>\n",
       "      <td>0.868387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemming</td>\n",
       "      <td>0.886452</td>\n",
       "      <td>0.873548</td>\n",
       "      <td>0.867097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemming</td>\n",
       "      <td>0.895484</td>\n",
       "      <td>0.876129</td>\n",
       "      <td>0.861935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>misspelled+stemmed</td>\n",
       "      <td>0.891613</td>\n",
       "      <td>0.863226</td>\n",
       "      <td>0.876129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>misspelled+lemmed</td>\n",
       "      <td>0.895484</td>\n",
       "      <td>0.876129</td>\n",
       "      <td>0.876129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stop_words_remove</td>\n",
       "      <td>0.869677</td>\n",
       "      <td>0.869677</td>\n",
       "      <td>0.868387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
